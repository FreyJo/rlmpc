@article{Gros2020,
  title = {Data-{{Driven Economic NMPC Using Reinforcement Learning}}},
  author = {Gros, S{\'e}bastien and Zanon, Mario},
  year = {2020},
  month = feb,
  journal = {IEEE Transactions on Automatic Control},
  volume = {65},
  number = {2},
  pages = {636--648},
  issn = {1558-2523},
  doi = {10.1109/TAC.2019.2913768},
  abstract = {Reinforcement learning (RL) is a powerful tool to perform data-driven optimal control without relying on a model of the system. However, RL struggles to provide hard guarantees on the behavior of the resulting control scheme. In contrast, nonlinear model predictive control (NMPC) and economic NMPC (ENMPC) are standard tools for the closed-loop optimal control of complex systems with constraints and limitations, and benefit from a rich theory to assess their closed-loop behavior. Unfortunately, the performance of (E)NMPC hinges on the quality of the model underlying the control scheme. In this paper, we show that an (E)NMPC scheme can be tuned to deliver the optimal policy of the real system even when using a wrong model. This result also holds for real systems having stochastic dynamics. This entails that ENMPC can be used as a new type of function approximator within RL. Furthermore, we investigate our results in the context of ENMPC and formally connect them to the concept of dissipativity, which is central for the ENMPC stability. Finally, we detail how these results can be used to deploy classic RL tools for tuning (E)NMPC schemes. We apply these tools on both, a classical linear MPC setting and a standard nonlinear example, from the ENMPC literature.},
  keywords = {Adaptation models,Adaptive nonlinear model predictive control (NMPC),economic NMPC (ENMPC),Economics,Optimal control,reinforcement learning,Reinforcement learning,Stability analysis,Stochastic processes,strict dissipativity,Tools},
  file = {/home/dirk/Zotero/storage/JX3KJT69/Gros2020 - Data Driven Economic NMPC Using Reinforcement Learning (1).pdf}
}

@inproceedings{Wenqi2021CDCgrid,
  title = {Optimal Management of the Peak Power Penalty for Smart Grids Using {{MPC-based}} Reinforcement Learning},
  booktitle = {2021 60th {{IEEE}} Conference on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Cai, Wenqi and Kordabad, Arash Bahari and Nejatbakhsh Esfahani, Hossein and Gros, S{\'e}bastien},
  year = {2021},
  publisher = {{IEEE}}
}

@inproceedings{Wenqi2021CDCShip,
  title = {{{MPC-based}} Reinforcement Learning for a Simplified Freight Mission of Autonomous Surface Vehicles},
  booktitle = {2021 60th {{IEEE}} Conference on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Cai, Wenqi and Kordabad, Arash Bahari and Nejatbakhsh Esfahani, Hossein and Lekkas, Anastasios M. and Gros, S{\'e}bastien},
  year = {2021},
  publisher = {{IEEE}}
}
